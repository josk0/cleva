{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4bf4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabpfn import TabPFNClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import experiments.runners as runners\n",
    "from data import load_us_perm_visas, load_credit_default\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "pfn_model = TabPFNClassifier(ignore_pretraining_limits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78fa902",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_us_perm_visas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cee9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, TargetEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import math\n",
    "from models.preprocessors import DatetimeFeatureSplitter, DatetimeFeatureEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4df4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RF Pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('datetime', DatetimeFeatureEncoder(), make_column_selector(dtype_include='datetime64[ns]')),\n",
    "        ('cat high c', TargetEncoder(), make_column_selector(dtype_include='object')),\n",
    "        ('num',SimpleImputer(strategy='constant', fill_value=0), make_column_selector(dtype_include=['int64', 'float64'])), # no need to scale numerical columns for RandomForest\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), make_column_selector(dtype_include='category'))\n",
    "        # ('pass', 'passthrough', make_column_selector(dtype_exclude=['datetime64']))\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    "    )        \n",
    "\n",
    "# Create classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', pfn_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d99cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TabPFN pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "            ('datetime', DatetimeFeatureSplitter(), make_column_selector(dtype_include='datetime64[ns]')),\n",
    "            ('num', SimpleImputer(strategy='mean'), make_column_selector(dtype_include=['int64', 'float64'])), # Here we see the limit of the approach: different strategy between models\n",
    "            ('cat', SimpleImputer(strategy='most_frequent'), make_column_selector(dtype_include='category')),\n",
    "            ('cat high cardinality', SimpleImputer(strategy='most_frequent'), make_column_selector(dtype_include='object')),\n",
    "            # todo: this isn't great, having to impute values for TabPFN. But the model otherwise had issues with missing variables. \n",
    "            # I think TabPFN expects missing values to be formatted in a certain way. Here it got an NA type or so in what it expected to be a str column\n",
    "\n",
    "        ],\n",
    "    remainder='passthrough'\n",
    "    )        \n",
    "\n",
    "# Create classifier pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', pfn_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7119ecfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size_for_model = 28000\n",
    "n_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec45e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import LearningCurveDisplay, StratifiedKFold # Use StratifiedKFold for classification\n",
    "\n",
    "# Define cross-validation strategy (e.g., 5-fold stratified)\n",
    "cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "n_samples = len(X)\n",
    "n_splits = cv.get_n_splits(X, y) # Get n_splits from the cv object\n",
    "\n",
    "# Calculate max size allowed by the CV split\n",
    "max_size_from_cv = int(n_samples * (1 - 1 / n_splits))\n",
    "\n",
    "# Determine the final max size, considering both CV and model limits\n",
    "final_max_train_size = min(max_size_for_model, max_size_from_cv)\n",
    "\n",
    "# Define train_sizes, ensuring it doesn't exceed the final_max_train_size\n",
    "# Example: 10 steps from 100 up to the calculated maximum\n",
    "start_size = 100\n",
    "num_steps = 10\n",
    "\n",
    "if final_max_train_size < start_size:\n",
    "    # Handle cases where even the smallest desired size is too large\n",
    "    # Maybe just use one size? Or raise an error?\n",
    "    train_sizes_abs = np.array([final_max_train_size])\n",
    "    print(f\"Warning: Max allowed train size ({final_max_train_size}) is less than start_size ({start_size}). Using only max size.\")\n",
    "else:\n",
    "    train_sizes_abs = np.linspace(start_size, final_max_train_size, num_steps, dtype=int)\n",
    "\n",
    "print(f\"Total samples: {n_samples}\")\n",
    "print(f\"CV folds: {n_splits}\")\n",
    "print(f\"Max training samples per fold: {max_size_from_cv}\")\n",
    "print(f\"Model/Config limit: {max_size_for_model}\")\n",
    "print(f\"Final max train size used in learning curve: {final_max_train_size}\")\n",
    "print(f\"Train sizes to be tested: {train_sizes_abs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2430114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore FutureWarning, of which TabPFNClassifier has a lot!\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3918f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating learning curve for TabPFN...\")\n",
    "# Use LearningCurveDisplay.from_estimator\n",
    "display = LearningCurveDisplay.from_estimator(\n",
    "    pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    train_sizes=train_sizes_abs, # Use absolute sizes suitable for TabPFN\n",
    "    scoring=\"f1\",  # Or \"roc_auc\", \"f1\", \"neg_log_loss\", etc. depending on your goal\n",
    "    n_jobs=1,  # Number of CPU cores to use\n",
    "    random_state=42, # Consistent state for any internal randomness if needed\n",
    "    # Add other relevant parameters if needed\n",
    ")\n",
    "\n",
    "display.ax_.set_title(f\"Learning Curve for TabPFN on Visa Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ee021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df09d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display.test_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes_abs = display.train_sizes\n",
    "train_scores_folds = display.train_scores # Shape: (n_train_sizes, n_folds)\n",
    "test_scores_folds = display.test_scores  # Shape: (n_train_sizes, n_folds)\n",
    "\n",
    "# 2. Determine the number of folds\n",
    "n_folds = train_scores_folds.shape[1]\n",
    "\n",
    "# 3. Prepare data for DataFrame construction\n",
    "data_list = []\n",
    "for i, size in enumerate(train_sizes_abs):\n",
    "    for fold in range(n_folds):\n",
    "        # Append training score data for this size and fold\n",
    "        data_list.append({\n",
    "            'train_size': size,\n",
    "            'fold': fold,\n",
    "            'score_type': 'train',\n",
    "            'score': train_scores_folds[i, fold]\n",
    "        })\n",
    "        # Append test score data for this size and fold\n",
    "        data_list.append({\n",
    "            'train_size': size,\n",
    "            'fold': fold,\n",
    "            'score_type': 'test',\n",
    "            'score': test_scores_folds[i, fold]\n",
    "        })\n",
    "\n",
    "# 4. Create pandas DataFrame\n",
    "df_learning_curve = pd.DataFrame(data_list)\n",
    "\n",
    "# 5. Export to CSV\n",
    "csv_filename = 'learning_curve_data.csv'\n",
    "df_learning_curve.to_csv(csv_filename, index=False)\n",
    "\n",
    "print(f\"Learning curve data exported to {csv_filename}\")\n",
    "print(\"\\nDataFrame head:\")\n",
    "print(df_learning_curve.head())\n",
    "print(\"\\nDataFrame tail:\")\n",
    "print(df_learning_curve.tail())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tabpfn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
